{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "image_shape = [28, 28, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read32(bytestream):\n",
    "    dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "    return np.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "\n",
    "def load_train_data():\n",
    "    mnist_dir = '/mnt/cube/f1fan/data/mnist'\n",
    "    train_image_path = os.path.join(mnist_dir, 'train-images-idx3-ubyte.gz')\n",
    "    train_label_path = os.path.join(mnist_dir, 'train-labels-idx1-ubyte.gz')\n",
    "\n",
    "    with gzip.open(train_image_path) as image_stream, gzip.open(train_label_path) as label_stream:\n",
    "        magic_image, magic_label = read32(image_stream), read32(label_stream)\n",
    "        if magic_image != 2051 or magic_label != 2049:\n",
    "            raise ValueError('Invalid magic number')\n",
    "\n",
    "        image_count, label_count = read32(image_stream), read32(label_stream)\n",
    "        row_count = read32(image_stream)\n",
    "        col_count = read32(image_stream)\n",
    "\n",
    "        label_buffer = label_stream.read(label_count)\n",
    "        train_labels = np.frombuffer(label_buffer, dtype=np.uint8)\n",
    "\n",
    "        image_buffer = image_stream.read(row_count * col_count * image_count)\n",
    "        train_images = np.frombuffer(image_buffer, dtype=np.uint8)\n",
    "        train_images = train_images.reshape(image_count, row_count, col_count, 1)\n",
    "\n",
    "        return train_images, train_labels\n",
    "\n",
    "def load_test_data():\n",
    "    mnist_dir = '/mnt/cube/f1fan/data/mnist'\n",
    "    test_image_path = os.path.join(mnist_dir, 't10k-images-idx3-ubyte.gz')\n",
    "    test_label_path = os.path.join(mnist_dir, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    with gzip.open(test_image_path) as image_stream, gzip.open(test_label_path) as label_stream:\n",
    "        magic_image, magic_label = read32(image_stream), read32(label_stream)\n",
    "        if magic_image != 2051 or magic_label != 2049:\n",
    "            raise ValueError('Invalid magic number')\n",
    "\n",
    "        image_count, label_count = read32(image_stream), read32(label_stream)\n",
    "        row_count = read32(image_stream)\n",
    "        col_count = read32(image_stream)\n",
    "\n",
    "        label_buffer = label_stream.read(label_count)\n",
    "        test_labels = np.frombuffer(label_buffer, dtype=np.uint8)\n",
    "\n",
    "        image_buffer = image_stream.read(row_count * col_count * image_count)\n",
    "        test_images = np.frombuffer(image_buffer, dtype=np.uint8)\n",
    "        test_images = test_images.reshape(image_count, row_count, col_count, 1)\n",
    "\n",
    "        return test_images, test_labels\n",
    "    \n",
    "def normalize(images):\n",
    "    '''\n",
    "    Normalize the intensity values from [0, 255] into [-1, 1].\n",
    "        images: Image array to normalize. Require each intensity value\n",
    "                ranges from 0 to 255.\n",
    "    Return normalized image array.\n",
    "    '''\n",
    "    return 1.0 * np.array(images) / 255 * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(scope, input_layer, output_dim, use_bias=True,\n",
    "            filter_size=3, strides=[1, 1, 1, 1]):\n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        conv_filter = tf.get_variable(\n",
    "            'conv_weight',\n",
    "            shape = [filter_size, filter_size, input_dim, output_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=0.0002)\n",
    "        )\n",
    "        conv = tf.nn.conv2d(input_layer, conv_filter, strides, 'SAME')\n",
    "\n",
    "        if use_bias:\n",
    "            bias = tf.get_variable(\n",
    "                'conv_bias',\n",
    "                shape = [output_dim],\n",
    "                dtype = tf.float32,\n",
    "                initializer = tf.constant_initializer(0.0)\n",
    "            )\n",
    "\n",
    "            output_layer = tf.nn.bias_add(conv, bias)\n",
    "            output_layer = tf.reshape(output_layer, conv.get_shape())\n",
    "        else:\n",
    "            output_layer = conv\n",
    "\n",
    "        return output_layer\n",
    "\n",
    "def batch_norm(scope, input_layer, is_training, reuse):\n",
    "\n",
    "    output_layer = tf.contrib.layers.batch_norm(\n",
    "        input_layer,\n",
    "        decay = 0.9,\n",
    "        scale = True,\n",
    "        epsilon = 1e-5,\n",
    "        is_training = is_training,\n",
    "        reuse = reuse,\n",
    "        scope = scope\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        input_dim = input_layer.get_shape().as_list()[-1]\n",
    "        mean, variance = tf.nn.moments(input_layer, [0, 1, 2])\n",
    "        beta = tf.get_variable(\n",
    "            'bn_beta',\n",
    "            shape = [input_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.constant_initializer(0.0)\n",
    "        )\n",
    "        gamma = tf.get_variable(\n",
    "            'bn_gamma',\n",
    "            shape = [input_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.constant_initializer(1.0)\n",
    "        )\n",
    "\n",
    "        output_layer = tf.nn.batch_normalization(input_layer, mean, variance,\n",
    "                                                 beta, gamma, 0.00001)\n",
    "    '''\n",
    "    return output_layer\n",
    "\n",
    "def lrelu(input_layer, leak=0.2):\n",
    "    #output_layer = tf.nn.relu(input_layer)\n",
    "    output_layer = tf.maximum(input_layer, leak * input_layer)\n",
    "    #output_layer = input_layer * tf.sigmoid(input_layer)\n",
    "    return output_layer\n",
    "\n",
    "def fully_connected(scope, input_layer, output_dim):\n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        fc_weight = tf.get_variable(\n",
    "            'fc_weight',\n",
    "            shape = [input_dim, output_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=0.0002)            \n",
    "        )\n",
    "\n",
    "        fc_bias = tf.get_variable(\n",
    "            'fc_bias',\n",
    "            shape = [output_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.constant_initializer(0.0)\n",
    "        )\n",
    "\n",
    "        output_layer = tf.matmul(input_layer, fc_weight) + fc_bias\n",
    "\n",
    "        return output_layer\n",
    "\n",
    "def avg_pool(scope, input_layer, ksize=None, strides=[1, 2, 2, 1]):\n",
    "    if ksize is None:\n",
    "        ksize = strides\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        output_layer = tf.nn.avg_pool(input_layer, ksize, strides, 'VALID')\n",
    "        return output_layer\n",
    "\n",
    "def residual(scope, input_layer, is_training, reuse,\n",
    "            increase_dim=False, first=False):\n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    if increase_dim:\n",
    "        output_dim = input_dim * 2\n",
    "        strides = [1, 2, 2, 1]\n",
    "    else:\n",
    "        output_dim = input_dim\n",
    "        strides = [1, 1, 1, 1]\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        if first:\n",
    "            h0 = input_layer\n",
    "        else:\n",
    "            h0_bn = batch_norm('h0_bn', input_layer, is_training, reuse)\n",
    "            h0 = lrelu(h0_bn)\n",
    "\n",
    "        h1_conv = conv2d('h1_conv', h0, output_dim, strides=strides)\n",
    "        h1_bn = batch_norm('h1_bn', h1_conv, is_training, reuse)\n",
    "        h1 = lrelu(h1_bn)\n",
    "\n",
    "        h2_conv = conv2d('h2_conv', h1, output_dim)\n",
    "        if increase_dim:\n",
    "            l = avg_pool('l_pool', input_layer)\n",
    "            l = tf.pad(l, [[0, 0], [0, 0], \n",
    "                           [0, 0], [input_dim // 2, input_dim // 2]])\n",
    "        else:\n",
    "            l = input_layer\n",
    "        h2 = tf.add(h2_conv, l)\n",
    "\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_image, is_training, reuse, num_units=5):\n",
    "    with tf.variable_scope('models', reuse=reuse):\n",
    "        init_dim = 16\n",
    "        batch_size = input_image.get_shape().as_list()[0]\n",
    "\n",
    "        r0_conv = conv2d('r0_conv', input_image, init_dim)\n",
    "        r0_bn = batch_norm('r0_bn', r0_conv, is_training, reuse)\n",
    "        r0 = lrelu(r0_bn)\n",
    "\n",
    "        r1_res = residual('res1.0', r0, is_training, reuse, first=True)\n",
    "        for k in xrange(1, num_units):\n",
    "            r1_res = residual('res1.{}'.format(k), r1_res, is_training, reuse)\n",
    "\n",
    "        r2_res = residual('res2.0', r1_res, is_training, reuse, increase_dim=True)\n",
    "        for k in xrange(1, num_units):\n",
    "            r2_res = residual('res2.{}'.format(k), r2_res, is_training, reuse)\n",
    "\n",
    "        r3_res = residual('res3.0', r2_res, is_training, reuse, increase_dim=True)\n",
    "        for k in xrange(1, num_units):\n",
    "            r3_res = residual('res3.{}'.format(k), r3_res, is_training, reuse)\n",
    "\n",
    "        r4_bn = batch_norm('r4_bn', r3_res, is_training, reuse)\n",
    "        r4 = lrelu(r4_bn)\n",
    "\n",
    "        axis = [1, 2]\n",
    "        r5 = tf.reduce_mean(\n",
    "            r4,\n",
    "            axis = axis\n",
    "        )\n",
    "\n",
    "        fc = fully_connected('fc', tf.reshape(r5, [batch_size, -1]), 10)\n",
    "        return tf.nn.softmax(fc), fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_op(batch_size, image_shape):\n",
    "    [height, width, channels] = image_shape\n",
    "    batch_shape = [batch_size, height, width, channels]\n",
    "    train_image_placeholder = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = batch_shape,\n",
    "        name = 'train_images'\n",
    "    )\n",
    "    train_label_placeholder = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape = [batch_size, ],\n",
    "        name = 'train_labels'\n",
    "    )\n",
    "    \n",
    "    prob, logits = build_model(train_image_placeholder, True, False)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = train_label_placeholder,\n",
    "        logits = logits\n",
    "    )\n",
    "    \n",
    "    train_step = tf.Variable(initial_value=0, trainable=False)\n",
    "    \n",
    "    decay = tf.train.exponential_decay(0.0002, train_step, 480000, 0.2, staircase=True)\n",
    "    decay_loss = []\n",
    "    for var in tf.trainable_variables():\n",
    "        if var.op.name.find(r'weight') > 0:\n",
    "            decay_loss.append(tf.nn.l2_loss(var))\n",
    "    prediction = tf.equal(tf.cast(tf.argmax(prob, axis=1), tf.int32), train_label_placeholder)\n",
    "    train_loss = tf.reduce_mean(loss) + tf.multiply(decay, tf.add_n(decay_loss))\n",
    "    train_accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    lr_boundaries = [4000, 12000, 24000]\n",
    "    lr_values = [0.1, 0.01, 0.001, 0.002]\n",
    "    lr = tf.train.piecewise_constant(train_step, lr_boundaries, lr_values)\n",
    "    \n",
    "    train_vars = [x for x in tf.trainable_variables() if 'models' in x.name]\n",
    "    optimizer = tf.train.MomentumOptimizer(lr, 0.9)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss, global_step=train_step, var_list=train_vars)\n",
    "    \n",
    "    return train_image_placeholder, train_label_placeholder, train_loss, train_accuracy, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_test_op(batch_size, image_shape):\n",
    "    [height, width, channels] = image_shape\n",
    "    batch_shape = [batch_size, height, width, channels]\n",
    "    test_image_placeholder = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = batch_shape,\n",
    "        name = 'test_images'\n",
    "    )\n",
    "    test_label_placeholder = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape = [batch_size, ],\n",
    "        name = 'test_labels'\n",
    "    )\n",
    "    \n",
    "    prob, logits = build_model(test_image_placeholder, False, True)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = test_label_placeholder,\n",
    "        logits = logits\n",
    "    )\n",
    "    prediction = tf.equal(tf.cast(tf.argmax(prob, axis=1), tf.int32), test_label_placeholder)\n",
    "    test_loss = tf.reduce_mean(loss)\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    \n",
    "    return test_image_placeholder, test_label_placeholder, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(sess):\n",
    "    train_images, train_labels = load_train_data()\n",
    "    test_images, test_labels = load_test_data()\n",
    "    \n",
    "    train_image_placeholder, train_label_placeholder, train_loss, train_accuracy, train_op = build_train_op(batch_size, image_shape)\n",
    "    test_image_placeholder, test_label_placeholder, test_loss, test_accuracy = build_test_op(batch_size, image_shape)\n",
    "\n",
    "    all_initializer_op = tf.global_variables_initializer()\n",
    "    sess.run(all_initializer_op)\n",
    "    \n",
    "    global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='')\n",
    "    print('Global variables:')\n",
    "    for i,var in enumerate(global_variables):\n",
    "        print('{}, {}, {}'.format(i, var.name, var.get_shape()))\n",
    "    \n",
    "    train_loss_list, train_accuracy_list = [], []\n",
    "    test_loss_list, test_accuracy_list = [], []\n",
    "    \n",
    "    for ep in xrange(epochs):\n",
    "        shuffle = np.random.permutation(train_images.shape[0])\n",
    "        \n",
    "        _loss = 0.0\n",
    "        _accuracy = 0.0\n",
    "        for bc in xrange(train_images.shape[0] // batch_size):\n",
    "            batch_images = normalize(train_images[shuffle[bc * batch_size : (bc + 1) * batch_size]])\n",
    "            batch_labels = train_labels[shuffle[bc * batch_size : (bc + 1) * batch_size]]\n",
    "            \n",
    "            sess.run(train_op, feed_dict={train_image_placeholder: batch_images, \n",
    "                                          train_label_placeholder: batch_labels})\n",
    "            batch_loss, batch_accuracy = sess.run([train_loss, train_accuracy],\n",
    "                                                  feed_dict={train_image_placeholder: batch_images,\n",
    "                                                             train_label_placeholder: batch_labels})\n",
    "            _loss += batch_loss\n",
    "            _accuracy += batch_accuracy\n",
    "        \n",
    "        _loss /= (train_images.shape[0] // batch_size)\n",
    "        _accuracy /= (train_images.shape[0] // batch_size)\n",
    "        train_loss_list.append(_loss)\n",
    "        train_accuracy_list.append(_accuracy)\n",
    "        \n",
    "        if ep % 5 == 0:\n",
    "            print ('Epoch {}:'.format(ep))\n",
    "            print ('    Train loss {}, Train accuracy {}'.format(_loss, _accuracy))\n",
    "\n",
    "        _loss = 0.0\n",
    "        _accuracy = 0.0\n",
    "        for bc in xrange(test_images.shape[0] // batch_size):\n",
    "            batch_images = normalize(test_images[bc * batch_size : (bc + 1) * batch_size])\n",
    "            batch_labels = test_labels[bc * batch_size : (bc + 1) * batch_size]\n",
    "            \n",
    "            batch_loss, batch_accuracy = sess.run([test_loss, test_accuracy],\n",
    "                                                  feed_dict={test_image_placeholder: batch_images,\n",
    "                                                             test_label_placeholder: batch_labels})\n",
    "            _loss += batch_loss\n",
    "            _accuracy += batch_accuracy\n",
    "        \n",
    "        _loss /= (test_images.shape[0] // batch_size)\n",
    "        _accuracy /= (test_images.shape[0] // batch_size)\n",
    "        test_loss_list.append(_loss)\n",
    "        test_accuracy_list.append(_accuracy)\n",
    "        if ep % 5 == 0:\n",
    "            print ('    Test loss {}, Test accuracy {}'.format(_loss, _accuracy))\n",
    "    \n",
    "    return train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global variables:\n",
      "0, ResNet/models/r0_conv/conv_weight:0, (3, 3, 1, 16)\n",
      "1, ResNet/models/r0_conv/conv_bias:0, (16,)\n",
      "2, ResNet/models/r0_bn/beta:0, (16,)\n",
      "3, ResNet/models/r0_bn/gamma:0, (16,)\n",
      "4, ResNet/models/r0_bn/moving_mean:0, (16,)\n",
      "5, ResNet/models/r0_bn/moving_variance:0, (16,)\n",
      "6, ResNet/models/res1.0/h1_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "7, ResNet/models/res1.0/h1_conv/conv_bias:0, (16,)\n",
      "8, ResNet/models/res1.0/h1_bn/beta:0, (16,)\n",
      "9, ResNet/models/res1.0/h1_bn/gamma:0, (16,)\n",
      "10, ResNet/models/res1.0/h1_bn/moving_mean:0, (16,)\n",
      "11, ResNet/models/res1.0/h1_bn/moving_variance:0, (16,)\n",
      "12, ResNet/models/res1.0/h2_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "13, ResNet/models/res1.0/h2_conv/conv_bias:0, (16,)\n",
      "14, ResNet/models/res1.1/h0_bn/beta:0, (16,)\n",
      "15, ResNet/models/res1.1/h0_bn/gamma:0, (16,)\n",
      "16, ResNet/models/res1.1/h0_bn/moving_mean:0, (16,)\n",
      "17, ResNet/models/res1.1/h0_bn/moving_variance:0, (16,)\n",
      "18, ResNet/models/res1.1/h1_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "19, ResNet/models/res1.1/h1_conv/conv_bias:0, (16,)\n",
      "20, ResNet/models/res1.1/h1_bn/beta:0, (16,)\n",
      "21, ResNet/models/res1.1/h1_bn/gamma:0, (16,)\n",
      "22, ResNet/models/res1.1/h1_bn/moving_mean:0, (16,)\n",
      "23, ResNet/models/res1.1/h1_bn/moving_variance:0, (16,)\n",
      "24, ResNet/models/res1.1/h2_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "25, ResNet/models/res1.1/h2_conv/conv_bias:0, (16,)\n",
      "26, ResNet/models/res1.2/h0_bn/beta:0, (16,)\n",
      "27, ResNet/models/res1.2/h0_bn/gamma:0, (16,)\n",
      "28, ResNet/models/res1.2/h0_bn/moving_mean:0, (16,)\n",
      "29, ResNet/models/res1.2/h0_bn/moving_variance:0, (16,)\n",
      "30, ResNet/models/res1.2/h1_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "31, ResNet/models/res1.2/h1_conv/conv_bias:0, (16,)\n",
      "32, ResNet/models/res1.2/h1_bn/beta:0, (16,)\n",
      "33, ResNet/models/res1.2/h1_bn/gamma:0, (16,)\n",
      "34, ResNet/models/res1.2/h1_bn/moving_mean:0, (16,)\n",
      "35, ResNet/models/res1.2/h1_bn/moving_variance:0, (16,)\n",
      "36, ResNet/models/res1.2/h2_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "37, ResNet/models/res1.2/h2_conv/conv_bias:0, (16,)\n",
      "38, ResNet/models/res1.3/h0_bn/beta:0, (16,)\n",
      "39, ResNet/models/res1.3/h0_bn/gamma:0, (16,)\n",
      "40, ResNet/models/res1.3/h0_bn/moving_mean:0, (16,)\n",
      "41, ResNet/models/res1.3/h0_bn/moving_variance:0, (16,)\n",
      "42, ResNet/models/res1.3/h1_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "43, ResNet/models/res1.3/h1_conv/conv_bias:0, (16,)\n",
      "44, ResNet/models/res1.3/h1_bn/beta:0, (16,)\n",
      "45, ResNet/models/res1.3/h1_bn/gamma:0, (16,)\n",
      "46, ResNet/models/res1.3/h1_bn/moving_mean:0, (16,)\n",
      "47, ResNet/models/res1.3/h1_bn/moving_variance:0, (16,)\n",
      "48, ResNet/models/res1.3/h2_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "49, ResNet/models/res1.3/h2_conv/conv_bias:0, (16,)\n",
      "50, ResNet/models/res1.4/h0_bn/beta:0, (16,)\n",
      "51, ResNet/models/res1.4/h0_bn/gamma:0, (16,)\n",
      "52, ResNet/models/res1.4/h0_bn/moving_mean:0, (16,)\n",
      "53, ResNet/models/res1.4/h0_bn/moving_variance:0, (16,)\n",
      "54, ResNet/models/res1.4/h1_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "55, ResNet/models/res1.4/h1_conv/conv_bias:0, (16,)\n",
      "56, ResNet/models/res1.4/h1_bn/beta:0, (16,)\n",
      "57, ResNet/models/res1.4/h1_bn/gamma:0, (16,)\n",
      "58, ResNet/models/res1.4/h1_bn/moving_mean:0, (16,)\n",
      "59, ResNet/models/res1.4/h1_bn/moving_variance:0, (16,)\n",
      "60, ResNet/models/res1.4/h2_conv/conv_weight:0, (3, 3, 16, 16)\n",
      "61, ResNet/models/res1.4/h2_conv/conv_bias:0, (16,)\n",
      "62, ResNet/models/res2.0/h0_bn/beta:0, (16,)\n",
      "63, ResNet/models/res2.0/h0_bn/gamma:0, (16,)\n",
      "64, ResNet/models/res2.0/h0_bn/moving_mean:0, (16,)\n",
      "65, ResNet/models/res2.0/h0_bn/moving_variance:0, (16,)\n",
      "66, ResNet/models/res2.0/h1_conv/conv_weight:0, (3, 3, 16, 32)\n",
      "67, ResNet/models/res2.0/h1_conv/conv_bias:0, (32,)\n",
      "68, ResNet/models/res2.0/h1_bn/beta:0, (32,)\n",
      "69, ResNet/models/res2.0/h1_bn/gamma:0, (32,)\n",
      "70, ResNet/models/res2.0/h1_bn/moving_mean:0, (32,)\n",
      "71, ResNet/models/res2.0/h1_bn/moving_variance:0, (32,)\n",
      "72, ResNet/models/res2.0/h2_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "73, ResNet/models/res2.0/h2_conv/conv_bias:0, (32,)\n",
      "74, ResNet/models/res2.1/h0_bn/beta:0, (32,)\n",
      "75, ResNet/models/res2.1/h0_bn/gamma:0, (32,)\n",
      "76, ResNet/models/res2.1/h0_bn/moving_mean:0, (32,)\n",
      "77, ResNet/models/res2.1/h0_bn/moving_variance:0, (32,)\n",
      "78, ResNet/models/res2.1/h1_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "79, ResNet/models/res2.1/h1_conv/conv_bias:0, (32,)\n",
      "80, ResNet/models/res2.1/h1_bn/beta:0, (32,)\n",
      "81, ResNet/models/res2.1/h1_bn/gamma:0, (32,)\n",
      "82, ResNet/models/res2.1/h1_bn/moving_mean:0, (32,)\n",
      "83, ResNet/models/res2.1/h1_bn/moving_variance:0, (32,)\n",
      "84, ResNet/models/res2.1/h2_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "85, ResNet/models/res2.1/h2_conv/conv_bias:0, (32,)\n",
      "86, ResNet/models/res2.2/h0_bn/beta:0, (32,)\n",
      "87, ResNet/models/res2.2/h0_bn/gamma:0, (32,)\n",
      "88, ResNet/models/res2.2/h0_bn/moving_mean:0, (32,)\n",
      "89, ResNet/models/res2.2/h0_bn/moving_variance:0, (32,)\n",
      "90, ResNet/models/res2.2/h1_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "91, ResNet/models/res2.2/h1_conv/conv_bias:0, (32,)\n",
      "92, ResNet/models/res2.2/h1_bn/beta:0, (32,)\n",
      "93, ResNet/models/res2.2/h1_bn/gamma:0, (32,)\n",
      "94, ResNet/models/res2.2/h1_bn/moving_mean:0, (32,)\n",
      "95, ResNet/models/res2.2/h1_bn/moving_variance:0, (32,)\n",
      "96, ResNet/models/res2.2/h2_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "97, ResNet/models/res2.2/h2_conv/conv_bias:0, (32,)\n",
      "98, ResNet/models/res2.3/h0_bn/beta:0, (32,)\n",
      "99, ResNet/models/res2.3/h0_bn/gamma:0, (32,)\n",
      "100, ResNet/models/res2.3/h0_bn/moving_mean:0, (32,)\n",
      "101, ResNet/models/res2.3/h0_bn/moving_variance:0, (32,)\n",
      "102, ResNet/models/res2.3/h1_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "103, ResNet/models/res2.3/h1_conv/conv_bias:0, (32,)\n",
      "104, ResNet/models/res2.3/h1_bn/beta:0, (32,)\n",
      "105, ResNet/models/res2.3/h1_bn/gamma:0, (32,)\n",
      "106, ResNet/models/res2.3/h1_bn/moving_mean:0, (32,)\n",
      "107, ResNet/models/res2.3/h1_bn/moving_variance:0, (32,)\n",
      "108, ResNet/models/res2.3/h2_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "109, ResNet/models/res2.3/h2_conv/conv_bias:0, (32,)\n",
      "110, ResNet/models/res2.4/h0_bn/beta:0, (32,)\n",
      "111, ResNet/models/res2.4/h0_bn/gamma:0, (32,)\n",
      "112, ResNet/models/res2.4/h0_bn/moving_mean:0, (32,)\n",
      "113, ResNet/models/res2.4/h0_bn/moving_variance:0, (32,)\n",
      "114, ResNet/models/res2.4/h1_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "115, ResNet/models/res2.4/h1_conv/conv_bias:0, (32,)\n",
      "116, ResNet/models/res2.4/h1_bn/beta:0, (32,)\n",
      "117, ResNet/models/res2.4/h1_bn/gamma:0, (32,)\n",
      "118, ResNet/models/res2.4/h1_bn/moving_mean:0, (32,)\n",
      "119, ResNet/models/res2.4/h1_bn/moving_variance:0, (32,)\n",
      "120, ResNet/models/res2.4/h2_conv/conv_weight:0, (3, 3, 32, 32)\n",
      "121, ResNet/models/res2.4/h2_conv/conv_bias:0, (32,)\n",
      "122, ResNet/models/res3.0/h0_bn/beta:0, (32,)\n",
      "123, ResNet/models/res3.0/h0_bn/gamma:0, (32,)\n",
      "124, ResNet/models/res3.0/h0_bn/moving_mean:0, (32,)\n",
      "125, ResNet/models/res3.0/h0_bn/moving_variance:0, (32,)\n",
      "126, ResNet/models/res3.0/h1_conv/conv_weight:0, (3, 3, 32, 64)\n",
      "127, ResNet/models/res3.0/h1_conv/conv_bias:0, (64,)\n",
      "128, ResNet/models/res3.0/h1_bn/beta:0, (64,)\n",
      "129, ResNet/models/res3.0/h1_bn/gamma:0, (64,)\n",
      "130, ResNet/models/res3.0/h1_bn/moving_mean:0, (64,)\n",
      "131, ResNet/models/res3.0/h1_bn/moving_variance:0, (64,)\n",
      "132, ResNet/models/res3.0/h2_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "133, ResNet/models/res3.0/h2_conv/conv_bias:0, (64,)\n",
      "134, ResNet/models/res3.1/h0_bn/beta:0, (64,)\n",
      "135, ResNet/models/res3.1/h0_bn/gamma:0, (64,)\n",
      "136, ResNet/models/res3.1/h0_bn/moving_mean:0, (64,)\n",
      "137, ResNet/models/res3.1/h0_bn/moving_variance:0, (64,)\n",
      "138, ResNet/models/res3.1/h1_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "139, ResNet/models/res3.1/h1_conv/conv_bias:0, (64,)\n",
      "140, ResNet/models/res3.1/h1_bn/beta:0, (64,)\n",
      "141, ResNet/models/res3.1/h1_bn/gamma:0, (64,)\n",
      "142, ResNet/models/res3.1/h1_bn/moving_mean:0, (64,)\n",
      "143, ResNet/models/res3.1/h1_bn/moving_variance:0, (64,)\n",
      "144, ResNet/models/res3.1/h2_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "145, ResNet/models/res3.1/h2_conv/conv_bias:0, (64,)\n",
      "146, ResNet/models/res3.2/h0_bn/beta:0, (64,)\n",
      "147, ResNet/models/res3.2/h0_bn/gamma:0, (64,)\n",
      "148, ResNet/models/res3.2/h0_bn/moving_mean:0, (64,)\n",
      "149, ResNet/models/res3.2/h0_bn/moving_variance:0, (64,)\n",
      "150, ResNet/models/res3.2/h1_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "151, ResNet/models/res3.2/h1_conv/conv_bias:0, (64,)\n",
      "152, ResNet/models/res3.2/h1_bn/beta:0, (64,)\n",
      "153, ResNet/models/res3.2/h1_bn/gamma:0, (64,)\n",
      "154, ResNet/models/res3.2/h1_bn/moving_mean:0, (64,)\n",
      "155, ResNet/models/res3.2/h1_bn/moving_variance:0, (64,)\n",
      "156, ResNet/models/res3.2/h2_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "157, ResNet/models/res3.2/h2_conv/conv_bias:0, (64,)\n",
      "158, ResNet/models/res3.3/h0_bn/beta:0, (64,)\n",
      "159, ResNet/models/res3.3/h0_bn/gamma:0, (64,)\n",
      "160, ResNet/models/res3.3/h0_bn/moving_mean:0, (64,)\n",
      "161, ResNet/models/res3.3/h0_bn/moving_variance:0, (64,)\n",
      "162, ResNet/models/res3.3/h1_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "163, ResNet/models/res3.3/h1_conv/conv_bias:0, (64,)\n",
      "164, ResNet/models/res3.3/h1_bn/beta:0, (64,)\n",
      "165, ResNet/models/res3.3/h1_bn/gamma:0, (64,)\n",
      "166, ResNet/models/res3.3/h1_bn/moving_mean:0, (64,)\n",
      "167, ResNet/models/res3.3/h1_bn/moving_variance:0, (64,)\n",
      "168, ResNet/models/res3.3/h2_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "169, ResNet/models/res3.3/h2_conv/conv_bias:0, (64,)\n",
      "170, ResNet/models/res3.4/h0_bn/beta:0, (64,)\n",
      "171, ResNet/models/res3.4/h0_bn/gamma:0, (64,)\n",
      "172, ResNet/models/res3.4/h0_bn/moving_mean:0, (64,)\n",
      "173, ResNet/models/res3.4/h0_bn/moving_variance:0, (64,)\n",
      "174, ResNet/models/res3.4/h1_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "175, ResNet/models/res3.4/h1_conv/conv_bias:0, (64,)\n",
      "176, ResNet/models/res3.4/h1_bn/beta:0, (64,)\n",
      "177, ResNet/models/res3.4/h1_bn/gamma:0, (64,)\n",
      "178, ResNet/models/res3.4/h1_bn/moving_mean:0, (64,)\n",
      "179, ResNet/models/res3.4/h1_bn/moving_variance:0, (64,)\n",
      "180, ResNet/models/res3.4/h2_conv/conv_weight:0, (3, 3, 64, 64)\n",
      "181, ResNet/models/res3.4/h2_conv/conv_bias:0, (64,)\n",
      "182, ResNet/models/r4_bn/beta:0, (64,)\n",
      "183, ResNet/models/r4_bn/gamma:0, (64,)\n",
      "184, ResNet/models/r4_bn/moving_mean:0, (64,)\n",
      "185, ResNet/models/r4_bn/moving_variance:0, (64,)\n",
      "186, ResNet/models/fc/fc_weight:0, (64, 10)\n",
      "187, ResNet/models/fc/fc_bias:0, (10,)\n",
      "188, ResNet/Variable:0, ()\n",
      "189, ResNet/ResNet/models/r0_conv/conv_weight/Momentum:0, (3, 3, 1, 16)\n",
      "190, ResNet/ResNet/models/r0_conv/conv_bias/Momentum:0, (16,)\n",
      "191, ResNet/ResNet/models/r0_bn/beta/Momentum:0, (16,)\n",
      "192, ResNet/ResNet/models/r0_bn/gamma/Momentum:0, (16,)\n",
      "193, ResNet/ResNet/models/res1.0/h1_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "194, ResNet/ResNet/models/res1.0/h1_conv/conv_bias/Momentum:0, (16,)\n",
      "195, ResNet/ResNet/models/res1.0/h1_bn/beta/Momentum:0, (16,)\n",
      "196, ResNet/ResNet/models/res1.0/h1_bn/gamma/Momentum:0, (16,)\n",
      "197, ResNet/ResNet/models/res1.0/h2_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "198, ResNet/ResNet/models/res1.0/h2_conv/conv_bias/Momentum:0, (16,)\n",
      "199, ResNet/ResNet/models/res1.1/h0_bn/beta/Momentum:0, (16,)\n",
      "200, ResNet/ResNet/models/res1.1/h0_bn/gamma/Momentum:0, (16,)\n",
      "201, ResNet/ResNet/models/res1.1/h1_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "202, ResNet/ResNet/models/res1.1/h1_conv/conv_bias/Momentum:0, (16,)\n",
      "203, ResNet/ResNet/models/res1.1/h1_bn/beta/Momentum:0, (16,)\n",
      "204, ResNet/ResNet/models/res1.1/h1_bn/gamma/Momentum:0, (16,)\n",
      "205, ResNet/ResNet/models/res1.1/h2_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "206, ResNet/ResNet/models/res1.1/h2_conv/conv_bias/Momentum:0, (16,)\n",
      "207, ResNet/ResNet/models/res1.2/h0_bn/beta/Momentum:0, (16,)\n",
      "208, ResNet/ResNet/models/res1.2/h0_bn/gamma/Momentum:0, (16,)\n",
      "209, ResNet/ResNet/models/res1.2/h1_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "210, ResNet/ResNet/models/res1.2/h1_conv/conv_bias/Momentum:0, (16,)\n",
      "211, ResNet/ResNet/models/res1.2/h1_bn/beta/Momentum:0, (16,)\n",
      "212, ResNet/ResNet/models/res1.2/h1_bn/gamma/Momentum:0, (16,)\n",
      "213, ResNet/ResNet/models/res1.2/h2_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "214, ResNet/ResNet/models/res1.2/h2_conv/conv_bias/Momentum:0, (16,)\n",
      "215, ResNet/ResNet/models/res1.3/h0_bn/beta/Momentum:0, (16,)\n",
      "216, ResNet/ResNet/models/res1.3/h0_bn/gamma/Momentum:0, (16,)\n",
      "217, ResNet/ResNet/models/res1.3/h1_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "218, ResNet/ResNet/models/res1.3/h1_conv/conv_bias/Momentum:0, (16,)\n",
      "219, ResNet/ResNet/models/res1.3/h1_bn/beta/Momentum:0, (16,)\n",
      "220, ResNet/ResNet/models/res1.3/h1_bn/gamma/Momentum:0, (16,)\n",
      "221, ResNet/ResNet/models/res1.3/h2_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "222, ResNet/ResNet/models/res1.3/h2_conv/conv_bias/Momentum:0, (16,)\n",
      "223, ResNet/ResNet/models/res1.4/h0_bn/beta/Momentum:0, (16,)\n",
      "224, ResNet/ResNet/models/res1.4/h0_bn/gamma/Momentum:0, (16,)\n",
      "225, ResNet/ResNet/models/res1.4/h1_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "226, ResNet/ResNet/models/res1.4/h1_conv/conv_bias/Momentum:0, (16,)\n",
      "227, ResNet/ResNet/models/res1.4/h1_bn/beta/Momentum:0, (16,)\n",
      "228, ResNet/ResNet/models/res1.4/h1_bn/gamma/Momentum:0, (16,)\n",
      "229, ResNet/ResNet/models/res1.4/h2_conv/conv_weight/Momentum:0, (3, 3, 16, 16)\n",
      "230, ResNet/ResNet/models/res1.4/h2_conv/conv_bias/Momentum:0, (16,)\n",
      "231, ResNet/ResNet/models/res2.0/h0_bn/beta/Momentum:0, (16,)\n",
      "232, ResNet/ResNet/models/res2.0/h0_bn/gamma/Momentum:0, (16,)\n",
      "233, ResNet/ResNet/models/res2.0/h1_conv/conv_weight/Momentum:0, (3, 3, 16, 32)\n",
      "234, ResNet/ResNet/models/res2.0/h1_conv/conv_bias/Momentum:0, (32,)\n",
      "235, ResNet/ResNet/models/res2.0/h1_bn/beta/Momentum:0, (32,)\n",
      "236, ResNet/ResNet/models/res2.0/h1_bn/gamma/Momentum:0, (32,)\n",
      "237, ResNet/ResNet/models/res2.0/h2_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "238, ResNet/ResNet/models/res2.0/h2_conv/conv_bias/Momentum:0, (32,)\n",
      "239, ResNet/ResNet/models/res2.1/h0_bn/beta/Momentum:0, (32,)\n",
      "240, ResNet/ResNet/models/res2.1/h0_bn/gamma/Momentum:0, (32,)\n",
      "241, ResNet/ResNet/models/res2.1/h1_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "242, ResNet/ResNet/models/res2.1/h1_conv/conv_bias/Momentum:0, (32,)\n",
      "243, ResNet/ResNet/models/res2.1/h1_bn/beta/Momentum:0, (32,)\n",
      "244, ResNet/ResNet/models/res2.1/h1_bn/gamma/Momentum:0, (32,)\n",
      "245, ResNet/ResNet/models/res2.1/h2_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "246, ResNet/ResNet/models/res2.1/h2_conv/conv_bias/Momentum:0, (32,)\n",
      "247, ResNet/ResNet/models/res2.2/h0_bn/beta/Momentum:0, (32,)\n",
      "248, ResNet/ResNet/models/res2.2/h0_bn/gamma/Momentum:0, (32,)\n",
      "249, ResNet/ResNet/models/res2.2/h1_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "250, ResNet/ResNet/models/res2.2/h1_conv/conv_bias/Momentum:0, (32,)\n",
      "251, ResNet/ResNet/models/res2.2/h1_bn/beta/Momentum:0, (32,)\n",
      "252, ResNet/ResNet/models/res2.2/h1_bn/gamma/Momentum:0, (32,)\n",
      "253, ResNet/ResNet/models/res2.2/h2_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "254, ResNet/ResNet/models/res2.2/h2_conv/conv_bias/Momentum:0, (32,)\n",
      "255, ResNet/ResNet/models/res2.3/h0_bn/beta/Momentum:0, (32,)\n",
      "256, ResNet/ResNet/models/res2.3/h0_bn/gamma/Momentum:0, (32,)\n",
      "257, ResNet/ResNet/models/res2.3/h1_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "258, ResNet/ResNet/models/res2.3/h1_conv/conv_bias/Momentum:0, (32,)\n",
      "259, ResNet/ResNet/models/res2.3/h1_bn/beta/Momentum:0, (32,)\n",
      "260, ResNet/ResNet/models/res2.3/h1_bn/gamma/Momentum:0, (32,)\n",
      "261, ResNet/ResNet/models/res2.3/h2_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "262, ResNet/ResNet/models/res2.3/h2_conv/conv_bias/Momentum:0, (32,)\n",
      "263, ResNet/ResNet/models/res2.4/h0_bn/beta/Momentum:0, (32,)\n",
      "264, ResNet/ResNet/models/res2.4/h0_bn/gamma/Momentum:0, (32,)\n",
      "265, ResNet/ResNet/models/res2.4/h1_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "266, ResNet/ResNet/models/res2.4/h1_conv/conv_bias/Momentum:0, (32,)\n",
      "267, ResNet/ResNet/models/res2.4/h1_bn/beta/Momentum:0, (32,)\n",
      "268, ResNet/ResNet/models/res2.4/h1_bn/gamma/Momentum:0, (32,)\n",
      "269, ResNet/ResNet/models/res2.4/h2_conv/conv_weight/Momentum:0, (3, 3, 32, 32)\n",
      "270, ResNet/ResNet/models/res2.4/h2_conv/conv_bias/Momentum:0, (32,)\n",
      "271, ResNet/ResNet/models/res3.0/h0_bn/beta/Momentum:0, (32,)\n",
      "272, ResNet/ResNet/models/res3.0/h0_bn/gamma/Momentum:0, (32,)\n",
      "273, ResNet/ResNet/models/res3.0/h1_conv/conv_weight/Momentum:0, (3, 3, 32, 64)\n",
      "274, ResNet/ResNet/models/res3.0/h1_conv/conv_bias/Momentum:0, (64,)\n",
      "275, ResNet/ResNet/models/res3.0/h1_bn/beta/Momentum:0, (64,)\n",
      "276, ResNet/ResNet/models/res3.0/h1_bn/gamma/Momentum:0, (64,)\n",
      "277, ResNet/ResNet/models/res3.0/h2_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "278, ResNet/ResNet/models/res3.0/h2_conv/conv_bias/Momentum:0, (64,)\n",
      "279, ResNet/ResNet/models/res3.1/h0_bn/beta/Momentum:0, (64,)\n",
      "280, ResNet/ResNet/models/res3.1/h0_bn/gamma/Momentum:0, (64,)\n",
      "281, ResNet/ResNet/models/res3.1/h1_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "282, ResNet/ResNet/models/res3.1/h1_conv/conv_bias/Momentum:0, (64,)\n",
      "283, ResNet/ResNet/models/res3.1/h1_bn/beta/Momentum:0, (64,)\n",
      "284, ResNet/ResNet/models/res3.1/h1_bn/gamma/Momentum:0, (64,)\n",
      "285, ResNet/ResNet/models/res3.1/h2_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "286, ResNet/ResNet/models/res3.1/h2_conv/conv_bias/Momentum:0, (64,)\n",
      "287, ResNet/ResNet/models/res3.2/h0_bn/beta/Momentum:0, (64,)\n",
      "288, ResNet/ResNet/models/res3.2/h0_bn/gamma/Momentum:0, (64,)\n",
      "289, ResNet/ResNet/models/res3.2/h1_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "290, ResNet/ResNet/models/res3.2/h1_conv/conv_bias/Momentum:0, (64,)\n",
      "291, ResNet/ResNet/models/res3.2/h1_bn/beta/Momentum:0, (64,)\n",
      "292, ResNet/ResNet/models/res3.2/h1_bn/gamma/Momentum:0, (64,)\n",
      "293, ResNet/ResNet/models/res3.2/h2_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "294, ResNet/ResNet/models/res3.2/h2_conv/conv_bias/Momentum:0, (64,)\n",
      "295, ResNet/ResNet/models/res3.3/h0_bn/beta/Momentum:0, (64,)\n",
      "296, ResNet/ResNet/models/res3.3/h0_bn/gamma/Momentum:0, (64,)\n",
      "297, ResNet/ResNet/models/res3.3/h1_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "298, ResNet/ResNet/models/res3.3/h1_conv/conv_bias/Momentum:0, (64,)\n",
      "299, ResNet/ResNet/models/res3.3/h1_bn/beta/Momentum:0, (64,)\n",
      "300, ResNet/ResNet/models/res3.3/h1_bn/gamma/Momentum:0, (64,)\n",
      "301, ResNet/ResNet/models/res3.3/h2_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "302, ResNet/ResNet/models/res3.3/h2_conv/conv_bias/Momentum:0, (64,)\n",
      "303, ResNet/ResNet/models/res3.4/h0_bn/beta/Momentum:0, (64,)\n",
      "304, ResNet/ResNet/models/res3.4/h0_bn/gamma/Momentum:0, (64,)\n",
      "305, ResNet/ResNet/models/res3.4/h1_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "306, ResNet/ResNet/models/res3.4/h1_conv/conv_bias/Momentum:0, (64,)\n",
      "307, ResNet/ResNet/models/res3.4/h1_bn/beta/Momentum:0, (64,)\n",
      "308, ResNet/ResNet/models/res3.4/h1_bn/gamma/Momentum:0, (64,)\n",
      "309, ResNet/ResNet/models/res3.4/h2_conv/conv_weight/Momentum:0, (3, 3, 64, 64)\n",
      "310, ResNet/ResNet/models/res3.4/h2_conv/conv_bias/Momentum:0, (64,)\n",
      "311, ResNet/ResNet/models/r4_bn/beta/Momentum:0, (64,)\n",
      "312, ResNet/ResNet/models/r4_bn/gamma/Momentum:0, (64,)\n",
      "313, ResNet/ResNet/models/fc/fc_weight/Momentum:0, (64, 10)\n",
      "314, ResNet/ResNet/models/fc/fc_bias/Momentum:0, (10,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "    Train loss 0.35549377126, Train accuracy 0.962016659478\n",
      "    Test loss 0.15153151013, Test accuracy 0.94909999609\n",
      "Epoch 5:\n",
      "    Train loss 0.112944253745, Train accuracy 0.994983303249\n",
      "    Test loss 0.0311196951172, Test accuracy 0.989799975157\n",
      "Epoch 10:\n",
      "    Train loss 0.0787041530634, Train accuracy 0.998566633463\n",
      "    Test loss 0.0106591246053, Test accuracy 0.996299970746\n",
      "Epoch 15:\n",
      "    Train loss 0.0690895357107, Train accuracy 0.99901663055\n",
      "    Test loss 0.0118776635325, Test accuracy 0.99619997263\n",
      "Epoch 20:\n",
      "    Train loss 0.0608111012727, Train accuracy 0.999549966653\n",
      "    Test loss 0.0110222764529, Test accuracy 0.996399965286\n",
      "Epoch 25:\n",
      "    Train loss 0.0596798373138, Train accuracy 0.999716634353\n",
      "    Test loss 0.0109360089619, Test accuracy 0.996399967074\n",
      "Epoch 30:\n",
      "    Train loss 0.0589057830721, Train accuracy 0.999716631472\n",
      "    Test loss 0.0113763425374, Test accuracy 0.996099968553\n",
      "Epoch 35:\n",
      "    Train loss 0.0581093228857, Train accuracy 0.999699964921\n",
      "    Test loss 0.0114444806376, Test accuracy 0.995999966264\n",
      "Epoch 40:\n",
      "    Train loss 0.0575004685856, Train accuracy 0.999733298421\n",
      "    Test loss 0.0115722326857, Test accuracy 0.996099970341\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
    "        with tf.device('/gpu:0'):\n",
    "            with tf.Session(config=config) as sess:\n",
    "                with tf.variable_scope('ResNet', reuse=None):\n",
    "                    train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list = main(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
