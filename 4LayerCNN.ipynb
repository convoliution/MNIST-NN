{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "image_shape = [28, 28, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read32(bytestream):\n",
    "    dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "    return np.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "\n",
    "def load_train_data():\n",
    "    mnist_dir = '/mnt/cube/f1fan/data/mnist'\n",
    "    train_image_path = os.path.join(mnist_dir, 'train-images-idx3-ubyte.gz')\n",
    "    train_label_path = os.path.join(mnist_dir, 'train-labels-idx1-ubyte.gz')\n",
    "\n",
    "    with gzip.open(train_image_path) as image_stream, gzip.open(train_label_path) as label_stream:\n",
    "        magic_image, magic_label = read32(image_stream), read32(label_stream)\n",
    "        if magic_image != 2051 or magic_label != 2049:\n",
    "            raise ValueError('Invalid magic number')\n",
    "\n",
    "        image_count, label_count = read32(image_stream), read32(label_stream)\n",
    "        row_count = read32(image_stream)\n",
    "        col_count = read32(image_stream)\n",
    "\n",
    "        label_buffer = label_stream.read(label_count)\n",
    "        train_labels = np.frombuffer(label_buffer, dtype=np.uint8)\n",
    "\n",
    "        image_buffer = image_stream.read(row_count * col_count * image_count)\n",
    "        train_images = np.frombuffer(image_buffer, dtype=np.uint8)\n",
    "        train_images = train_images.reshape(image_count, row_count, col_count, 1)\n",
    "\n",
    "        return train_images, train_labels\n",
    "\n",
    "def load_test_data():\n",
    "    mnist_dir = '/mnt/cube/f1fan/data/mnist'\n",
    "    test_image_path = os.path.join(mnist_dir, 't10k-images-idx3-ubyte.gz')\n",
    "    test_label_path = os.path.join(mnist_dir, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    with gzip.open(test_image_path) as image_stream, gzip.open(test_label_path) as label_stream:\n",
    "        magic_image, magic_label = read32(image_stream), read32(label_stream)\n",
    "        if magic_image != 2051 or magic_label != 2049:\n",
    "            raise ValueError('Invalid magic number')\n",
    "\n",
    "        image_count, label_count = read32(image_stream), read32(label_stream)\n",
    "        row_count = read32(image_stream)\n",
    "        col_count = read32(image_stream)\n",
    "\n",
    "        label_buffer = label_stream.read(label_count)\n",
    "        test_labels = np.frombuffer(label_buffer, dtype=np.uint8)\n",
    "\n",
    "        image_buffer = image_stream.read(row_count * col_count * image_count)\n",
    "        test_images = np.frombuffer(image_buffer, dtype=np.uint8)\n",
    "        test_images = test_images.reshape(image_count, row_count, col_count, 1)\n",
    "\n",
    "        return test_images, test_labels\n",
    "    \n",
    "def normalize(images):\n",
    "    '''\n",
    "    Normalize the intensity values from [0, 255] into [-1, 1].\n",
    "        images: Image array to normalize. Require each intensity value\n",
    "                ranges from 0 to 255.\n",
    "    Return normalized image array.\n",
    "    '''\n",
    "    return 1.0 * np.array(images) / 255 * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(scope, input_layer, output_dim, use_bias=True,\n",
    "            filter_size=3, strides=[1, 1, 1, 1]):\n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        conv_filter = tf.get_variable(\n",
    "            'conv_weight',\n",
    "            shape = [filter_size, filter_size, input_dim, output_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=0.0002)\n",
    "        )\n",
    "        conv = tf.nn.conv2d(input_layer, conv_filter, strides, 'SAME')\n",
    "\n",
    "        if use_bias:\n",
    "            bias = tf.get_variable(\n",
    "                'conv_bias',\n",
    "                shape = [output_dim],\n",
    "                dtype = tf.float32,\n",
    "                initializer = tf.constant_initializer(0.0)\n",
    "            )\n",
    "\n",
    "            output_layer = tf.nn.bias_add(conv, bias)\n",
    "            output_layer = tf.reshape(output_layer, conv.get_shape())\n",
    "        else:\n",
    "            output_layer = conv\n",
    "\n",
    "        return output_layer\n",
    "\n",
    "def batch_norm(scope, input_layer, is_training, reuse):\n",
    "\n",
    "    output_layer = tf.contrib.layers.batch_norm(\n",
    "        input_layer,\n",
    "        decay = 0.9,\n",
    "        scale = True,\n",
    "        epsilon = 1e-5,\n",
    "        is_training = is_training,\n",
    "        reuse = reuse,\n",
    "        scope = scope\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        input_dim = input_layer.get_shape().as_list()[-1]\n",
    "        mean, variance = tf.nn.moments(input_layer, [0, 1, 2])\n",
    "        beta = tf.get_variable(\n",
    "            'bn_beta',\n",
    "            shape = [input_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.constant_initializer(0.0)\n",
    "        )\n",
    "        gamma = tf.get_variable(\n",
    "            'bn_gamma',\n",
    "            shape = [input_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.constant_initializer(1.0)\n",
    "        )\n",
    "\n",
    "        output_layer = tf.nn.batch_normalization(input_layer, mean, variance,\n",
    "                                                 beta, gamma, 0.00001)\n",
    "    '''\n",
    "    return output_layer\n",
    "\n",
    "def lrelu(input_layer, leak=0.2):\n",
    "    #output_layer = tf.nn.relu(input_layer)\n",
    "    output_layer = tf.maximum(input_layer, leak * input_layer)\n",
    "    #output_layer = input_layer * tf.sigmoid(input_layer)\n",
    "    return output_layer\n",
    "\n",
    "def fully_connected(scope, input_layer, output_dim):\n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        fc_weight = tf.get_variable(\n",
    "            'fc_weight',\n",
    "            shape = [input_dim, output_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=0.0002)            \n",
    "        )\n",
    "\n",
    "        fc_bias = tf.get_variable(\n",
    "            'fc_bias',\n",
    "            shape = [output_dim],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.constant_initializer(0.0)\n",
    "        )\n",
    "\n",
    "        output_layer = tf.matmul(input_layer, fc_weight) + fc_bias\n",
    "\n",
    "        return output_layer\n",
    "\n",
    "def avg_pool(scope, input_layer, ksize=None, strides=[1, 2, 2, 1]):\n",
    "    if ksize is None:\n",
    "        ksize = strides\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        output_layer = tf.nn.avg_pool(input_layer, ksize, strides, 'VALID')\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_image, is_training, reuse):\n",
    "    with tf.variable_scope('models', reuse=reuse):\n",
    "        filter_dim = 64\n",
    "        batch_size = input_image.get_shape().as_list()[0]\n",
    "\n",
    "        h0_conv = conv2d('h0_conv', input_image, filter_dim, filter_size=5, strides=[1, 2, 2, 1])\n",
    "        h0 = lrelu(h0_conv)\n",
    "\n",
    "        h1_conv = conv2d('h1_conv', h0, filter_dim * 2, filter_size=5, strides=[1, 2, 2, 1])\n",
    "        h1_bn = batch_norm('h1_bn', h1_conv, is_training, reuse)\n",
    "        h1 = lrelu(h1_bn)\n",
    "\n",
    "        h2_conv = conv2d('h2_conv', h1, filter_dim * 4, filter_size=5, strides=[1, 2, 2, 1])\n",
    "        h2_bn = batch_norm('h2_bn', h2_conv, is_training, reuse)\n",
    "        h2 = lrelu(h2_bn)\n",
    "\n",
    "        h3_conv = conv2d('h3_conv', h2, filter_dim * 8, filter_size=5, strides=[1, 2, 2, 1])\n",
    "        h3_bn = batch_norm('h3_bn', h3_conv, is_training, reuse)\n",
    "        h3 = lrelu(h3_bn)\n",
    "\n",
    "        fc = fully_connected('fc', tf.reshape(h3, [batch_size, -1]), 10)\n",
    "        return tf.nn.softmax(fc), fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_op(batch_size, image_shape):\n",
    "    [height, width, channels] = image_shape\n",
    "    batch_shape = [batch_size, height, width, channels]\n",
    "    train_image_placeholder = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = batch_shape,\n",
    "        name = 'train_images'\n",
    "    )\n",
    "    train_label_placeholder = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape = [batch_size, ],\n",
    "        name = 'train_labels'\n",
    "    )\n",
    "    \n",
    "    prob, logits = build_model(train_image_placeholder, True, False)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = train_label_placeholder,\n",
    "        logits = logits\n",
    "    )\n",
    "    \n",
    "    train_step = tf.Variable(initial_value=0, trainable=False)\n",
    "    prediction = tf.equal(tf.cast(tf.argmax(prob, axis=1), tf.int32), train_label_placeholder)\n",
    "    train_loss = tf.reduce_mean(loss)\n",
    "    train_accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "    train_vars = [x for x in tf.trainable_variables() if 'models' in x.name]\n",
    "    optimizer = tf.train.AdamOptimizer(0.01)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss, global_step=train_step, var_list=train_vars)\n",
    "    \n",
    "    return train_image_placeholder, train_label_placeholder, train_loss, train_accuracy, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_test_op(batch_size, image_shape):\n",
    "    [height, width, channels] = image_shape\n",
    "    batch_shape = [batch_size, height, width, channels]\n",
    "    test_image_placeholder = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape = batch_shape,\n",
    "        name = 'test_images'\n",
    "    )\n",
    "    test_label_placeholder = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape = [batch_size, ],\n",
    "        name = 'test_labels'\n",
    "    )\n",
    "    \n",
    "    prob, logits = build_model(test_image_placeholder, False, True)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = test_label_placeholder,\n",
    "        logits = logits\n",
    "    )\n",
    "    prediction = tf.equal(tf.cast(tf.argmax(prob, axis=1), tf.int32), test_label_placeholder)\n",
    "    test_loss = tf.reduce_mean(loss)\n",
    "    test_accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    \n",
    "    return test_image_placeholder, test_label_placeholder, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(sess):\n",
    "    train_images, train_labels = load_train_data()\n",
    "    test_images, test_labels = load_test_data()\n",
    "    \n",
    "    train_image_placeholder, train_label_placeholder, train_loss, train_accuracy, train_op = build_train_op(batch_size, image_shape)\n",
    "    test_image_placeholder, test_label_placeholder, test_loss, test_accuracy = build_test_op(batch_size, image_shape)\n",
    "\n",
    "    all_initializer_op = tf.global_variables_initializer()\n",
    "    sess.run(all_initializer_op)\n",
    "    \n",
    "    global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='')\n",
    "    print('Global variables:')\n",
    "    for i,var in enumerate(global_variables):\n",
    "        print('{}, {}, {}'.format(i, var.name, var.get_shape()))\n",
    "    \n",
    "    train_loss_list, train_accuracy_list = [], []\n",
    "    test_loss_list, test_accuracy_list = [], []\n",
    "    \n",
    "    for ep in xrange(epochs):\n",
    "        shuffle = np.random.permutation(train_images.shape[0])\n",
    "        \n",
    "        _loss = 0.0\n",
    "        _accuracy = 0.0\n",
    "        for bc in xrange(train_images.shape[0] // batch_size):\n",
    "            batch_images = normalize(train_images[shuffle[bc * batch_size : (bc + 1) * batch_size]])\n",
    "            batch_labels = train_labels[shuffle[bc * batch_size : (bc + 1) * batch_size]]\n",
    "            \n",
    "            sess.run(train_op, feed_dict={train_image_placeholder: batch_images, \n",
    "                                          train_label_placeholder: batch_labels})\n",
    "            batch_loss, batch_accuracy = sess.run([train_loss, train_accuracy],\n",
    "                                                  feed_dict={train_image_placeholder: batch_images,\n",
    "                                                             train_label_placeholder: batch_labels})\n",
    "            _loss += batch_loss\n",
    "            _accuracy += batch_accuracy\n",
    "        \n",
    "        _loss /= (train_images.shape[0] // batch_size)\n",
    "        _accuracy /= (train_images.shape[0] // batch_size)\n",
    "        train_loss_list.append(_loss)\n",
    "        train_accuracy_list.append(_accuracy)\n",
    "        \n",
    "        if ep % 5 == 0:\n",
    "            print ('Epoch {}:'.format(ep))\n",
    "            print ('    Train loss {}, Train accuracy {}'.format(_loss, _accuracy))\n",
    "\n",
    "        _loss = 0.0\n",
    "        _accuracy = 0.0\n",
    "        for bc in xrange(test_images.shape[0] // batch_size):\n",
    "            batch_images = normalize(test_images[bc * batch_size : (bc + 1) * batch_size])\n",
    "            batch_labels = test_labels[bc * batch_size : (bc + 1) * batch_size]\n",
    "            \n",
    "            batch_loss, batch_accuracy = sess.run([test_loss, test_accuracy],\n",
    "                                                  feed_dict={test_image_placeholder: batch_images,\n",
    "                                                             test_label_placeholder: batch_labels})\n",
    "            _loss += batch_loss\n",
    "            _accuracy += batch_accuracy\n",
    "        \n",
    "        _loss /= (test_images.shape[0] // batch_size)\n",
    "        _accuracy /= (test_images.shape[0] // batch_size)\n",
    "        test_loss_list.append(_loss)\n",
    "        test_accuracy_list.append(_accuracy)\n",
    "        if ep % 5 == 0:\n",
    "            print ('    Test loss {}, Test accuracy {}'.format(_loss, _accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global variables:\n",
      "0, SimpleCNN/models/h0_conv/conv_weight:0, (5, 5, 1, 64)\n",
      "1, SimpleCNN/models/h0_conv/conv_bias:0, (64,)\n",
      "2, SimpleCNN/models/h1_conv/conv_weight:0, (5, 5, 64, 128)\n",
      "3, SimpleCNN/models/h1_conv/conv_bias:0, (128,)\n",
      "4, SimpleCNN/models/h1_bn/beta:0, (128,)\n",
      "5, SimpleCNN/models/h1_bn/gamma:0, (128,)\n",
      "6, SimpleCNN/models/h1_bn/moving_mean:0, (128,)\n",
      "7, SimpleCNN/models/h1_bn/moving_variance:0, (128,)\n",
      "8, SimpleCNN/models/h2_conv/conv_weight:0, (5, 5, 128, 256)\n",
      "9, SimpleCNN/models/h2_conv/conv_bias:0, (256,)\n",
      "10, SimpleCNN/models/h2_bn/beta:0, (256,)\n",
      "11, SimpleCNN/models/h2_bn/gamma:0, (256,)\n",
      "12, SimpleCNN/models/h2_bn/moving_mean:0, (256,)\n",
      "13, SimpleCNN/models/h2_bn/moving_variance:0, (256,)\n",
      "14, SimpleCNN/models/h3_conv/conv_weight:0, (5, 5, 256, 512)\n",
      "15, SimpleCNN/models/h3_conv/conv_bias:0, (512,)\n",
      "16, SimpleCNN/models/h3_bn/beta:0, (512,)\n",
      "17, SimpleCNN/models/h3_bn/gamma:0, (512,)\n",
      "18, SimpleCNN/models/h3_bn/moving_mean:0, (512,)\n",
      "19, SimpleCNN/models/h3_bn/moving_variance:0, (512,)\n",
      "20, SimpleCNN/models/fc/fc_weight:0, (2048, 10)\n",
      "21, SimpleCNN/models/fc/fc_bias:0, (10,)\n",
      "22, SimpleCNN/Variable:0, ()\n",
      "23, SimpleCNN/beta1_power:0, ()\n",
      "24, SimpleCNN/beta2_power:0, ()\n",
      "25, SimpleCNN/SimpleCNN/models/h0_conv/conv_weight/Adam:0, (5, 5, 1, 64)\n",
      "26, SimpleCNN/SimpleCNN/models/h0_conv/conv_weight/Adam_1:0, (5, 5, 1, 64)\n",
      "27, SimpleCNN/SimpleCNN/models/h0_conv/conv_bias/Adam:0, (64,)\n",
      "28, SimpleCNN/SimpleCNN/models/h0_conv/conv_bias/Adam_1:0, (64,)\n",
      "29, SimpleCNN/SimpleCNN/models/h1_conv/conv_weight/Adam:0, (5, 5, 64, 128)\n",
      "30, SimpleCNN/SimpleCNN/models/h1_conv/conv_weight/Adam_1:0, (5, 5, 64, 128)\n",
      "31, SimpleCNN/SimpleCNN/models/h1_conv/conv_bias/Adam:0, (128,)\n",
      "32, SimpleCNN/SimpleCNN/models/h1_conv/conv_bias/Adam_1:0, (128,)\n",
      "33, SimpleCNN/SimpleCNN/models/h1_bn/beta/Adam:0, (128,)\n",
      "34, SimpleCNN/SimpleCNN/models/h1_bn/beta/Adam_1:0, (128,)\n",
      "35, SimpleCNN/SimpleCNN/models/h1_bn/gamma/Adam:0, (128,)\n",
      "36, SimpleCNN/SimpleCNN/models/h1_bn/gamma/Adam_1:0, (128,)\n",
      "37, SimpleCNN/SimpleCNN/models/h2_conv/conv_weight/Adam:0, (5, 5, 128, 256)\n",
      "38, SimpleCNN/SimpleCNN/models/h2_conv/conv_weight/Adam_1:0, (5, 5, 128, 256)\n",
      "39, SimpleCNN/SimpleCNN/models/h2_conv/conv_bias/Adam:0, (256,)\n",
      "40, SimpleCNN/SimpleCNN/models/h2_conv/conv_bias/Adam_1:0, (256,)\n",
      "41, SimpleCNN/SimpleCNN/models/h2_bn/beta/Adam:0, (256,)\n",
      "42, SimpleCNN/SimpleCNN/models/h2_bn/beta/Adam_1:0, (256,)\n",
      "43, SimpleCNN/SimpleCNN/models/h2_bn/gamma/Adam:0, (256,)\n",
      "44, SimpleCNN/SimpleCNN/models/h2_bn/gamma/Adam_1:0, (256,)\n",
      "45, SimpleCNN/SimpleCNN/models/h3_conv/conv_weight/Adam:0, (5, 5, 256, 512)\n",
      "46, SimpleCNN/SimpleCNN/models/h3_conv/conv_weight/Adam_1:0, (5, 5, 256, 512)\n",
      "47, SimpleCNN/SimpleCNN/models/h3_conv/conv_bias/Adam:0, (512,)\n",
      "48, SimpleCNN/SimpleCNN/models/h3_conv/conv_bias/Adam_1:0, (512,)\n",
      "49, SimpleCNN/SimpleCNN/models/h3_bn/beta/Adam:0, (512,)\n",
      "50, SimpleCNN/SimpleCNN/models/h3_bn/beta/Adam_1:0, (512,)\n",
      "51, SimpleCNN/SimpleCNN/models/h3_bn/gamma/Adam:0, (512,)\n",
      "52, SimpleCNN/SimpleCNN/models/h3_bn/gamma/Adam_1:0, (512,)\n",
      "53, SimpleCNN/SimpleCNN/models/fc/fc_weight/Adam:0, (2048, 10)\n",
      "54, SimpleCNN/SimpleCNN/models/fc/fc_weight/Adam_1:0, (2048, 10)\n",
      "55, SimpleCNN/SimpleCNN/models/fc/fc_bias/Adam:0, (10,)\n",
      "56, SimpleCNN/SimpleCNN/models/fc/fc_bias/Adam_1:0, (10,)\n",
      "Epoch 0:\n",
      "    Train loss 0.256138350082, Train accuracy 0.944833329394\n",
      "    Test loss 0.0844857207022, Test accuracy 0.97319999218\n",
      "Epoch 5:\n",
      "    Train loss 0.017959207891, Train accuracy 0.995433309178\n",
      "    Test loss 0.0440901686425, Test accuracy 0.985799978971\n",
      "Epoch 10:\n",
      "    Train loss 0.0104755575445, Train accuracy 0.997466643949\n",
      "    Test loss 0.0462666067756, Test accuracy 0.987999985218\n",
      "Epoch 15:\n",
      "    Train loss 0.00590788024293, Train accuracy 0.998599976103\n",
      "    Test loss 0.0547509006936, Test accuracy 0.98769997716\n",
      "Epoch 20:\n",
      "    Train loss 0.00328196232529, Train accuracy 0.999249977469\n",
      "    Test loss 0.0540769663951, Test accuracy 0.989999986291\n",
      "Epoch 25:\n",
      "    Train loss 0.00302958486951, Train accuracy 0.999333309035\n",
      "    Test loss 0.117068740157, Test accuracy 0.981299991608\n",
      "Epoch 30:\n",
      "    Train loss 0.00150541911376, Train accuracy 0.99963330855\n",
      "    Test loss 0.073202273027, Test accuracy 0.990099980235\n",
      "Epoch 35:\n",
      "    Train loss 0.00210982046281, Train accuracy 0.999516641597\n",
      "    Test loss 0.0819257495231, Test accuracy 0.989199978709\n",
      "Epoch 40:\n",
      "    Train loss 0.00137773228221, Train accuracy 0.999749976595\n",
      "    Test loss 0.0686551705912, Test accuracy 0.991499979496\n",
      "Epoch 45:\n",
      "    Train loss 0.00174664647708, Train accuracy 0.999633309642\n",
      "    Test loss 0.0745987190789, Test accuracy 0.990699979663\n",
      "Epoch 50:\n",
      "    Train loss 0.000409094916018, Train accuracy 0.999916638831\n",
      "    Test loss 0.0923817440226, Test accuracy 0.989099984169\n",
      "Epoch 55:\n",
      "    Train loss 0.00128386231229, Train accuracy 0.99974997431\n",
      "    Test loss 0.0862148810958, Test accuracy 0.991399981976\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu)\n",
    "        with tf.device('/gpu:0'):\n",
    "            with tf.Session(config=config) as sess:\n",
    "                with tf.variable_scope('SimpleCNN', reuse=None):\n",
    "                    main(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
